<!DOCTYPE html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <script src="https://aframe.io/releases/1.4.2/aframe.min.js"></script>

    <script src="https://unpkg.com/networked-aframe@^0.11.0/dist/networked-aframe.min.js"></script>
    <script src="https://webrtc.github.io/adapter/adapter-latest.js" crossorigin="anonymous"></script>
    <script src="/dist/naf-janus-adapter.js"></script>

    <style>
        @font-face {
            font-family: 'Karla';
            font-style: normal;
            font-weight: 400;
            src: local('Karla'), local('Karla-Regular'),
                url(https://fonts.gstatic.com/s/karla/v5/31P4mP32i98D9CEnGyeX9Q.woff2) format('woff2');
            unicode-range: U+0100-024F, U+1E00-1EFF, U+20A0-20AB, U+20AD-20CF, U+2C60-2C7F, U+A720-A7FF;
        }

        @font-face {
            font-family: 'Karla';
            font-style: normal;
            font-weight: 400;
            src: local('Karla'), local('Karla-Regular'),
                url(https://fonts.gstatic.com/s/karla/v5/Zi_e6rBgGqv33BWF8WTq8g.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074,
                U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
        }

        button.webvr-ui-button {
            font-family: 'Karla', sans-serif;

            border: rgb(80, 168, 252) 2px solid;
            border-radius: 2px;
            box-sizing: border-box;
            background: none;

            height: 55px;
            min-width: 175.99999999999997px;
            display: inline-block;
            position: relative;

            cursor: pointer;
        }

        button.webvr-ui-button:focus {
            outline: none;
        }

        /*
        * Logo
        */

        .webvr-ui-logo {
            width: 55px;
            height: 55px;
            position: absolute;
            top: 0px;
            left: 0px;
            width: 51px;
            height: 51px;
        }

        .webvr-ui-svg {
            fill: rgb(80, 168, 252);
            margin-top: 18.166666666666668px;
            margin-left: 18.333333333333332px;
        }

        .webvr-ui-svg-error {
            fill: rgb(80, 168, 252);
            display: none;
            margin-top: 14.092592592592595px;
            margin-left: 18.333333333333332px;
        }


        /*
        * Title
        */

        .webvr-ui-title {
            color: rgb(80, 168, 252);
            position: relative;
            font-size: 18.333333333333332px;
            padding-left: 57.75px;
            padding-right: 18.333333333333332px;
        }

        /*
        * disabled
        */

        button.webvr-ui-button[disabled=true] {
            opacity: 0.5;
        }

        button.webvr-ui-button[disabled=true]>.webvr-ui-logo>.webvr-ui-svg {
            display: none;
        }

        button.webvr-ui-button[disabled=true]>.webvr-ui-logo>.webvr-ui-svg-error {
            display: initial;
        }

        /*
        * warning
        */
        div.webxrWarning {
            color: #f00;
            font-weight: bold;
        }
    </style>

    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">

    <title>CoWebXR Client</title>

    <link href="./AR Depth API_files/common.css" rel="stylesheet">

    <script src="./js/webxr-polyfill.js"></script>
    <script src="./js/webxr-button.js"></script>
</head>

<body>
    <header>
        <summary>CoWebXR - Client Application</summary>
        <p>
            This is the client application component of the CoWebXR system.
        </p>

    </header>

    <div id="text-overlay">
        <div id="text-info"></div>
    </div>

    <a-scene vr-mode-ui="enabled: false;" '
        renderer="logarithmicDepthBuffer: true;" 
        embedded
        arjs="
            trackingMethod: best; 
            sourceType: webcam;
            debugUIEnabled: false;" 
        networked-scene="
            room: 1;
            debug: false;
            adapter: janus;
            connectOnLoad: true;
            serverURL: wss://cowebxr.com:8989/janus;">

            <a-assets>
                <!-- Templates -->
        
                <!-- Avatar -->
                <template id="avatar-template" type="text/html">
                    <a-entity class="avatar" networked-audio-source>
                        <a-plane color="#fff" width="4" height="3" position="0 .6 0" material="side: back"
                            networked-video-source></a-plane>
                        <a-sphere class="head" color="#5985ff" scale="0.45 0.5 0.4"></a-sphere>
                        <a-entity class="face" position="0 0.05 0">
                            <a-sphere class="eye" color="#efefef" position="0.16 0.1 -0.35" scale="0.12 0.12 0.12">
                                <a-sphere class="pupil" color="#000" position="0 0 -1" scale="0.2 0.2 0.2"></a-sphere>
                            </a-sphere>
                            <a-sphere class="eye" color="#efefef" position="-0.16 0.1 -0.35" scale="0.12 0.12 0.12">
                                <a-sphere class="pupil" color="#000" position="0 0 -1" scale="0.2 0.2 0.2"></a-sphere>
                            </a-sphere>
                        </a-entity>
                    </a-entity>
                </template>
        
                <!-- /Templates -->
            </a-assets>

        
        <a-camera look-controls="enabled: false" wasd-controls="enabled: false" position="0.5 0.5 -3.5" fit id="mainCamera">
            <a-entity id="player" networked="template:#avatar-template;attachTemplateToLocal:false;" position="0 0 -1"
                rotation="0 0 0" onhover>
            </a-entity>
        </a-camera>
    </a-scene>


    <script id="vertexShader" type="x-shader/x-vertex">
        precision mediump float;
  
        attribute vec2 aVertexPosition;
        attribute vec2 aTexCoord;
  
        varying vec2 vTexCoord;
  
        void main(void) {
          gl_Position = vec4(aVertexPosition, 0.0, 1.0);
          vTexCoord = aTexCoord;
        }
    </script>

    <script id="fragmentShader" type="x-shader/x-fragment" src="./AR Depth API_files/depth-api-gpu.frag"></script>
    <script id="turboFragment" type="x-shader/x-fragment" src="./AR Depth API_files/turbo.glsl"></script>

    <script type="module">
        import { mat4, vec3, mat3, vec2 } from './js/render/math/gl-matrix.js';
        // import * as mat4 from "../js/third-party/gl-matrix/mat4.js"

        // XR globals. 
        let xrButton = null;
        let xrRefSpace = null;

        // WebGL scene globals. 
        let gl = null;
        let glBinding = null;

        let shaderProgram = null;
        let programInfo = null;
        let vertexBuffer = null;
        let depthTexture = null;

        let buffers = null;
        let texture = null;
        let readback_framebuffer = null;
        let readback_pixels = null;

        let rgb_data = null;

        const perform_readback = true;

        // shader code 
        let vertexShaderSource = null;
        let fragmentShaderSource = null;

        // variables to store data 
        let device_orientation = null;

        const textOverlayElement = document.querySelector("#text-overlay");
        if (!textOverlayElement) {
            console.error("#text-overlay element not found!");
            throw new Error("#text-overlay element not found!");
        }

        const textInfoElement = document.querySelector("#text-info");
        if (!textInfoElement) {
            console.error("#text-info element not found!");
            throw new Error("#text-info element not found!");
        }

        function initXR() {
            xrButton = new XRDeviceButton({
                onRequestSession: onRequestSession,
                onEndSession: onEndSession,
                textEnterXRTitle: "START AR",
                textXRNotFoundTitle: "AR NOT FOUND",
                textExitXRTitle: "EXIT AR",
                supportedSessionTypes: ['immersive-ar']
            });
            document.querySelector('header').appendChild(xrButton.domElement);
        }

        function onRequestSession() {
            // Requests an immersive session with environment integration.

            let options = {
                requiredFeatures: ['depth-sensing', 'dom-overlay', 'camera-access'],
                optionalFeatures: [],
                domOverlay: { root: textOverlayElement },
                depthSensing: {
                    usagePreference: ["cpu-optimized"],
                    dataFormatPreference: ["luminance-alpha"],
                }
            };

            navigator.xr.requestSession('immersive-ar', options).then((session) => {
                session.mode = 'immersive-ar';
                xrButton.setSession(session);

                fetchShaders().then(() => {
                    onSessionStarted(session);
                });
            });
        }

        function onSessionStarted(session) {

            session.addEventListener('end', onSessionEnded);

            let canvas = document.createElement('canvas');
            gl = canvas.getContext('webgl', {
                xrCompatible: true
            });

            glBinding = new XRWebGLBinding(session, gl);

            initializeGL();

            session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl) });
            session.requestReferenceSpace('local').then((refSpace) => {
                xrRefSpace = refSpace;
                session.requestAnimationFrame(onXRFrame);
            });

            if (session.depthUsage != "cpu-optimized") {
                throw new Error("Unsupported depth API usage!");
            }

            if (session.depthDataFormat != "luminance-alpha") {
                throw new Error("Unsupported depth data format!");
            }
        }

        function onEndSession(session) {
            session.end();
        }

        function onSessionEnded(event) {
            xrButton.setSession(null);
        }

        async function fetchShader(id) {
            const element = document.getElementById(id);
            const url = element.src;

            const response = await fetch(url);
            const text = await response.text();

            return text;
        }

        async function fetchShaders() {
            vertexShaderSource = document.getElementById('vertexShader').textContent;
            fragmentShaderSource = await fetchShader("fragmentShader") + "\n" + await fetchShader("turboFragment");
        }

        function uploadVertexData(vertices) {
            const result = gl.createBuffer();

            gl.bindBuffer(gl.ARRAY_BUFFER, result);
            gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW);
            gl.bindBuffer(gl.ARRAY_BUFFER, null);

            return result;
        }

        function initializeGL() {
            shaderProgram = initShaderProgram(vertexShaderSource, fragmentShaderSource);

            programInfo = {
                program: shaderProgram,
                attribLocations: {
                    vertexPosition: gl.getAttribLocation(shaderProgram, 'aVertexPosition'),
                    texCoord: gl.getAttribLocation(shaderProgram, 'aTexCoord'),
                },
                uniformLocations: {
                    depthTexture: gl.getUniformLocation(shaderProgram, 'uDepthTexture'),
                    uvTransform: gl.getUniformLocation(shaderProgram, 'uUvTransform'),
                    rawValueToMeters: gl.getUniformLocation(shaderProgram, 'uRawValueToMeters'),
                    alpha: gl.getUniformLocation(shaderProgram, 'uAlpha'),
                },
            };

            // clip space coordinates + texture space coordinates
            // our depth buffer has an origin in top-left corner of the screen -
            // we need to adjust the texture coordinates to account for that
            const vertices_data = [
                -1, -1, 0, 1, // bottom left
                1, -1, 1, 1, // bottom right
                -1, 1, 0, 0, // top left
                1, 1, 1, 0, // top right
            ];

            vertexBuffer = uploadVertexData(vertices_data);

            depthTexture = gl.createTexture();
            gl.bindTexture(gl.TEXTURE_2D, depthTexture);

            // depth texture will likely not be power-of-2-sized, set parameters
            // that would still make it work, see
            // https://www.khronos.org/webgl/wiki/WebGL_and_OpenGL_Differences#Non-Power_of_Two_Texture_Support
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);

            gl.bindTexture(gl.TEXTURE_2D, null);
        }

        function initShaderProgram(vsSource, fsSource) {
            const vertexShader = loadShader(gl.VERTEX_SHADER, vsSource);
            const fragmentShader = loadShader(gl.FRAGMENT_SHADER, fsSource);

            // Create the shader program
            const shaderProgram = gl.createProgram();
            gl.attachShader(shaderProgram, vertexShader);
            gl.attachShader(shaderProgram, fragmentShader);
            gl.linkProgram(shaderProgram);

            // If creating the shader program failed, alert
            if (!gl.getProgramParameter(shaderProgram, gl.LINK_STATUS)) {
                alert("Unable to initialize the shader program: " +
                    gl.getProgramInfoLog(shaderProgram)
                );
                return null;
            }

            return shaderProgram;
        }

        function loadShader(type, source) {
            const shader = gl.createShader(type);

            gl.shaderSource(shader, source);
            gl.compileShader(shader);

            if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
                alert(
                    "An error occurred compiling the shaders: " +
                    gl.getShaderInfoLog(shader)
                );
                gl.deleteShader(shader);
                return null;
            }

            return shader;
        }

        const options = {
            enableHighAccuracy: true,
            timeout: 5000,
            maximumAge: 0,
        };

        function gps_success(pos) {
            const crd = pos.coords;
            // setting current device position
            device_posn = {
                altitude: crd.altitude,
                latitude: crd.latitude,
                longitude: crd.longitude,
                accuracy: crd.accuracy
            };
        }

        function gps_error(err) {
            console.warn(`ERROR(${err.code}): ${err.message}`);
        }

        // A function to get the compass direction from an angle
        function getCompassDirection(angle) {
            // An array of directions
            var directions = ["N", "NE", "E", "SE", "S", "SW", "W", "NW", "N"];
            // The index of the direction in the array
            var index = Math.round((angle + 22.5) / 45) % 8;
            // Return the direction
            return directions[index];
        }

        // A function to handle the deviceorientation event
        function handle_orientation(event) {
            // Get the angle of the device relative to the magnetic north
            var angle = event.alpha;
            device_orientation = angle;

            // Get the compass direction from the angle
            // var direction = getCompassDirection(angle);
            // Display the direction on the screen
            // console.log("compass direction", direction);
        }

        const intrinsicsPrinted = {};

        function getCameraIntrinsics(projectionMatrix, viewport) {
            const p = projectionMatrix;
            // Principal point in pixels (typically at or near the center of the viewport)
            let u0 = (1 - p[8]) * viewport.width / 2 + viewport.x;
            let v0 = (1 - p[9]) * viewport.height / 2 + viewport.y;
            // Focal lengths in pixels (these are equal for square pixels)
            let ax = viewport.width / 2 * p[0];
            let ay = viewport.height / 2 * p[5];
            // Skew factor in pixels (nonzero for rhomboid pixels)
            let gamma = viewport.width / 2 * p[4];

            // Print the calculated intrinsics, but once per unique value to
            // avoid log spam. These can change every frame for some XR devices.
            const intrinsicString = (
                "intrinsics: u0=" +u0 + " v0=" + v0 + " ax=" + ax + " ay=" + ay +
                    " gamma=" + gamma + " for viewport {width=" +
                    viewport.width + ",height=" + viewport.height + ",x=" +
                    viewport.x + ",y=" + viewport.y + "}");
            if (!intrinsicsPrinted[intrinsicString]) {
                console.log("projection:", Array.from(projectionMatrix).join(", "));
                console.log(intrinsicString);
                intrinsicsPrinted[intrinsicString] = true;
            }
        }

        // Called every time a XRSession requests that a new frame be drawn.
        function onXRFrame(t, frame) {
            const session = frame.session;
            session.requestAnimationFrame(onXRFrame);

            const baseLayer = session.renderState.baseLayer;
            const pose = frame.getViewerPose(xrRefSpace);

            let rgb_data = null;

            if (pose) {
                gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer);

                // Clear the framebuffer
                gl.clearColor(0, 0, 0, 0);
                gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

                gl.enable(gl.DEPTH_TEST);

                for (const view of pose.views) {
                    const viewport = baseLayer.getViewport(view);
                    gl.viewport(viewport.x, viewport.y,
                        viewport.width, viewport.height);

                    // For an application working in viewport space, get the camera intrinsics
                    // based on the viewport dimensions:
                    getCameraIntrinsics(view.projectionMatrix, viewport);

                    if (view.camera) {
                        const cameraViewport = {
                          width: view.camera.width,
                          height: view.camera.height,
                          x: 0,
                          y: 0};
                      getCameraIntrinsics(view.projectionMatrix, cameraViewport);

                        // Update camera image texture.
                        const texture = glBinding.getCameraImage(view.camera);
                        const texture_bytes = view.camera.width * view.camera.height * 4;
                        if (!readback_pixels || readback_pixels.length != texture_bytes) {
                        readback_pixels = new Uint8Array(texture_bytes);
                        }

                        readback_pixels.fill(0);
                        readback_framebuffer = gl.createFramebuffer();

                        gl.bindTexture(gl.TEXTURE_2D, texture);
                        gl.bindFramebuffer(gl.FRAMEBUFFER, readback_framebuffer);
                        gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, texture, 0);

                        if (gl.checkFramebufferStatus(gl.FRAMEBUFFER) == gl.FRAMEBUFFER_COMPLETE) {
                            gl.readPixels(0, 0, view.camera.width, view.camera.height, gl.RGBA, gl.UNSIGNED_BYTE, readback_pixels);

                            // Create a canvas to hold the pixel data
                            const canvas = document.createElement('canvas');
                            canvas.width = view.camera.width;
                            canvas.height = view.camera.height;
                            const context = canvas.getContext('2d');

                            const imageData = context.createImageData(view.camera.width, view.camera.height);
                            imageData.data.set(readback_pixels);

                            // Put the ImageData onto the canvas
                            context.putImageData(imageData, 0, 0);

                            const jpegQuality = 0.7; // Adjust JPEG quality as needed
                            const jpegDataURL = canvas.toDataURL("image/jpeg", jpegQuality);

                            const base64Data = jpegDataURL.split(",")[1];
                            const binaryData = atob(base64Data);
                            const jpegUint8Array = new Uint8Array(binaryData.length);
                            for (let i = 0; i < binaryData.length; i++) {
                                jpegUint8Array[i] = binaryData.charCodeAt(i);
                            }

                            rgb_data = jpegUint8Array;

                            // console.log(jpegUint8Array);

                            // console.log("PIXELS", view.camera.width, view.camera.height, readback_pixels.length)
                        } else {
                        console.warn("Framebuffer incomplete!");
                        }

                        // gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer);
                    }

                    const depthData = frame.getDepthInformation(view);

                    // Attempting to export the data
                    // console.log(depthData);
                    // const depthInMeters = depthData.getDepthInMeters(0.25, 0.75);
                    // console.log("Depth at normalized view coordinates (0.25, 0.75) is:",
                    // depthInMeters);

                    navigator.geolocation.getCurrentPosition(gps_success, gps_error, options);
                    window.addEventListener("deviceorientation", handle_orientation);

                    const depth_uint16 = new Uint16Array(depthData.data);
                    const depth_json_array = Array.from(depth_uint16);

                    const rgb_json_array = Array.from(rgb_data);


                    curr_frame_data = {
                        rgb_data: {
                            width: view.camera.width, 
                            height: view.camera.height,
                            data: rgb_json_array 
                        },
                        depth_data: {
                            width: depthData.width,
                            height: depthData.height,
                            data: depth_json_array
                        },
                        gps_data: device_posn
                    }

                    //

                    if (depthData) {
                        textInfoElement.innerHTML = "";

                        // renderDepthInformationGPU(depthData, view, viewport);
                    } else {
                        textInfoElement.innerHTML = "Depth data unavailable in the current frame!";
                    }
                }
            } else {
                textInfoElement.innerHTML = "Pose unavailable in the current frame!";
            }
        }

        function renderDepthInformationGPU(depthData, view, viewport) {
            const depth_width = depthData.width;
            const depth_height = depthData.height;

            gl.useProgram(programInfo.program);

            gl.bindBuffer(gl.ARRAY_BUFFER, vertexBuffer);

            gl.vertexAttribPointer(
                programInfo.attribLocations.vertexPosition,
                2, // 2 components
                gl.FLOAT,
                false, // don't normalize
                16, // stride = 4 floats * 4 bytes
                0 // start at offset 0 of the buffer
            );
            gl.enableVertexAttribArray(
                programInfo.attribLocations.vertexPosition
            );

            gl.vertexAttribPointer(
                programInfo.attribLocations.texCoord,
                2, // 2 components
                gl.FLOAT,
                false, // don't normalize
                16, // stride = 4 floats * 4 bytes
                8 // start at offset of 2 floats * 4 bytes of the buffer
            );
            gl.enableVertexAttribArray(
                programInfo.attribLocations.texCoord
            );

            gl.bindTexture(gl.TEXTURE_2D, depthTexture);

            // Supply the data buffer after converting it to Uint8Array - the
            // gl.texImage2D expects Uint8Array when using gl.UNSIGNED_BYTE type.
            gl.texImage2D(gl.TEXTURE_2D, 0, gl.LUMINANCE_ALPHA, depthData.width,
                depthData.height, 0, gl.LUMINANCE_ALPHA, gl.UNSIGNED_BYTE,
                new Uint8Array(depthData.data));

            gl.activeTexture(gl.TEXTURE0);
            gl.uniform1i(programInfo.uniformLocations.depthTexture, 0);

            gl.uniformMatrix4fv(programInfo.uniformLocations.uvTransform, false,
                depthData.normDepthBufferFromNormView.matrix);

            gl.uniform1f(programInfo.uniformLocations.rawValueToMeters,
                depthData.rawValueToMeters);

            gl.uniform1f(programInfo.uniformLocations.alpha,
                0.75);

            gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
        }

        // Start the XR application.
        initXR();
    </script>

    
<script>
    function genClientId() {
        let num = '';
        for (let i = 0; i < 16; i++) {
            num += Math.floor(Math.random() * 10).toString();
        }
        return num;
    }

    // On mobile remove elements that are resource heavy
    var isMobile = AFRAME.utils.device.isMobile();
    document.addEventListener('DOMContentLoaded', () => {
        const scene = document.querySelector('a-scene');
        scene.addEventListener('adapter-ready', ({ detail: adapter }) => {
            const clientId = genClientId();
            console.log("CLIENT ID", clientId);
            adapter.setClientId(clientId);
            navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
                adapter.setLocalMediaStream(stream).then(() => {
                    adapter.enableMicrophone(true);
                    const currentStream = stream;
                    navigator.mediaDevices.getUserMedia(
                        {
                            video: {
                                mediaSource: "camera",
                                // width: { max: 1280, ideal: 4096 },
                                // height: { ideal: 2160 },
                                // frameRate: { ideal: 60 },
                                facingMode: { ideal: "environment" }
                            }
                        }).then((stream) => {
                            currentStream.addTrack(stream.getVideoTracks()[0]);
                            adapter.setLocalMediaStream(currentStream);
                        });
                });
            }).catch(err => {
                console.warn("Microphone access not allowed. This client will not broadcast audio.");
            });
        });
    });

</script>

<script>
    let device_posn = null;
    let curr_frame_data = null;

    var sceneEl = document.querySelector('a-scene');
    var entityEl = document.createElement('a-entity');
    entityEl.setAttribute('geometry', {
        primitive: 'plane',
        height: 0.5,
        width: 0.25,
    });
    sceneEl.appendChild(entityEl);

    var textEl = document.createElement('a-entity');
    textEl.setAttribute('text', {
        value: '',
        height: 1,
        width: 1
    });
    textEl.setAttribute('id', "recognition");
    textEl.setAttribute('position', { x: 0, y: 0, z: -3 });
    sceneEl.appendChild(textEl);

    function pushData(senderId, dataType, data, targetObj) {
        // console.log(data);
        // console.log(data[0].results.names);

        // const returned_data = JSON.parse(this.responseText);
        // const recognition_results = JSON.parse(returned_data[0].results);

        const yolo_width = 480;
        const yolo_height = 640;

        const win_width = window.innerWidth;
        const win_height = window.innerHeight;

        const recognition_results = data[0].results
        const num_rr = recognition_results.bb.length;

        // // console.log("window size " + window.innerWidth + "x" + window.innerHeight);

        if (num_rr > 0) {
            var textElement = document.getElementById("recognition");
            textElement.innerHTML = '';
            for (var i = 0; i < num_rr; i++) {
                const curr_label = recognition_results.names[i];
                const curr_res = recognition_results.bb[i];

                const x_min = curr_res[0];
                const y_min = curr_res[1];
                const box_width = curr_res[2];
                const box_height = curr_res[3];

                // console.log(curr_res, curr_label);

                // const x_ratio = win_width / yolo_width;
                // const y_ratio = win_height / yolo_height;

                // const x_centre = (x_min + box_width) / 2;
                // const y_centre = (y_min + box_height) / 2;

                // const x_cen_new = (x_centre*x_ratio) - x_ratio;
                // const y_cen_new = (y_centre*y_ratio) - y_ratio;

                const x_centre = x_min + (box_width / 2);
                const y_centre = y_min - (box_height / 2);

                const y_min_new = box_height - y_min - 1;

                // console.log(x_centre, y_centre);

                var res_box = document.createElement('a-entity');
                // res_box.setAttribute('geometry', {
                //     primitive: 'plane',
                //     height: box_height,
                //     width: box_width
                // });
                // // res_box.setAttribute('rotation', {x: 0, y: 0, z: -90});
                // res_box.setAttribute('material', {
                //     opacity: 0,
                //     transparent: true
                // });
                res_box.setAttribute('position', { x: x_min, y: y_min, z: -2.5 });
                // res_box.setAttribute('rotation', {x: 180, y: 0, z: 0});

                var new_el = document.createElement('a-entity');
                new_el.setAttribute('text', {
                    value: curr_label,
                    height: 1,
                    width: 1
                });
                new_el.setAttribute('id', "recognition");
                new_el.setAttribute('position', { x: x_min, y: y_min, z: -1 });
                // new_el.setAttribute('rotation', {x: 50, y: 30, z: 0});
                new_el.setAttribute('material', {
                    side: "double"
                })
                res_box.appendChild(new_el);
                textElement.appendChild(res_box);

            }
        }

    }

    // Define custom schema for syncing avatar color, set by random-color
    NAF.schemas.add({
        template: '#avatar-template',
        components: [
            'position',
            'rotation',
            {
                selector: '.head',
                component: 'material',
                property: 'color'
            }
        ]
    });

    // creating secure websocket 
    let ws = new WebSocket('wss://10.38.151.146:8765'); 
    ws.onopen = () => {
        console.log('[STATUS] Secure WebSocket connection made with Reconstructor Service');
        ws.send("Initial Registration");
    };

    ws.onmessage = (message) => {
        console.log(`message received`, message.data);
        // ws.close();
    };

    function send_data() {
        NAF.connection.broadcastDataGuaranteed('reliableBroadcast', '');
        NAF.connection.broadcastData('unreliableBroadcast', '');

        var clients = NAF.connection.getConnectedClients();
        // console.log("CLIENTS", clients);

        var firstClient;
        for (firstClient in clients) break;

        // console.error(clients);
        // console.error(firstClient);

        NAF.connection.sendDataGuaranteed(8379079273475150, 'reliableSend', '');
        NAF.connection.sendData(8379079273475150, 'unreliableSend', '');

        console.log("sending data", curr_frame_data);
        let ws = new WebSocket('wss://10.38.151.146:8765'); 
        // ws.binaryType = "arraybuffer"
        ws.onopen = () => {
            // console.log("sending data to server", new Date())
            // const depth_json_string = JSON.stringify(depth_json_array);
            ws.send(JSON.stringify(curr_frame_data));
        };
    }

    // Called by Networked-Aframe when connected to server
    function onConnect() {
        console.log("onConnect", new Date());
        NAF.connection.subscribeToDataChannel('results', pushData);
        setInterval(send_data, 250);
    }
</script>
</body>